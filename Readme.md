# ğŸ› ï¸ Projeto de Pipeline Lakehouse com Apache Airflow e PySpark

Este repositÃ³rio contÃ©m um pipeline de dados desenvolvido com Apache Airflow e PySpark, estruturado com base nas camadas **Bronze**, **Silver** e **Gold** de um modelo Lakehouse.

## ğŸ“ Estrutura do Projeto
<pre>
.
â”œâ”€â”€ dados/                         # Camadas de dados no formato Data Lakehouse
â”‚   â”œâ”€â”€ landing/                   # Dados brutos recebidos (JSON)
â”‚   â”‚   â”œâ”€â”€ customers.json
â”‚   â”‚   â”œâ”€â”€ order_item.json
â”‚   â”‚   â””â”€â”€ orders.json
â”‚   â”œâ”€â”€ bronze/                    # Dados brutos convertidos para Parquet
â”‚   â”‚   â”œâ”€â”€ customers_bronze.parquet/
â”‚   â”‚   â”œâ”€â”€ order_items_bronze.parquet/
â”‚   â”‚   â””â”€â”€ orders_bronze.parquet/
â”‚   â”œâ”€â”€ silver/                    # Dados limpos e estruturados
â”‚   â”‚   â”œâ”€â”€ customers_silver.parquet/
â”‚   â”‚   â”œâ”€â”€ order_items_silver.parquet/
â”‚   â”‚   â””â”€â”€ orders_silver.parquet/
â”‚   â””â”€â”€ gold/                      # Dados prontos para anÃ¡lise e dashboards
â”‚       â””â”€â”€ pedidos_por_cidade_estado/
â”œâ”€â”€ dags/                          # Pipelines do Airflow
â”‚   â”œâ”€â”€ pipeline_lakehouse.py      # Pipeline principal
â”‚   â””â”€â”€ __pycache__/               # Arquivos compilados do Python (gerados automaticamente)
â”œâ”€â”€ scripts/                       # Scripts de transformaÃ§Ã£o por camada
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ bronze/
â”‚   â”‚   â”œâ”€â”€ customers_bronze.py
â”‚   â”‚   â”œâ”€â”€ order_items_bronze.py
â”‚   â”‚   â””â”€â”€ orders_bronze.py
â”‚   â”œâ”€â”€ silver/
â”‚   â”‚   â”œâ”€â”€ customers_silver.py
â”‚   â”‚   â”œâ”€â”€ order_items_silver.py
â”‚   â”‚   â””â”€â”€ orders_silver.py
â”‚   â””â”€â”€ gold/
â”‚       â””â”€â”€ processar_gold.py
</pre>

## ğŸ—‚ï¸ Estrutura do Pipeline

<pre>
pipeline_lakehouse
â”œâ”€â”€ bronze_customers
â”‚     â””â”€â”€ silver_customers
â”‚           â””â”€â”€
â”œâ”€â”€ bronze_orders
â”‚     â””â”€â”€ silver_orders
â”‚           â””â”€â”€
â”œâ”€â”€ bronze_order_items
â”‚     â””â”€â”€ silver_order_items
â”‚           â””â”€â”€
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ gold
</pre>

## ğŸ”„ Pipeline de OrquestraÃ§Ã£o (`pipeline_lakehouse.py`)

A DAG principal `pipeline_lakehouse` executa as trÃªs etapas principais do fluxo:

1. **Camada Landing â†’ Bronze (`bronze_customers`, `bronze_orders`, `bronze_order_items`)**:  
   - LÃª arquivos JSON da camada *Landing*.
   - Converte para formato Parquet.
   - Salva os dados tratados na camada *Bronze*.

2. **Camada Bronze â†’ Silver (`silver_customers`, `silver_orders`, `silver_order_items`)**:  
   - LÃª os dados em Parquet da camada Bronze.
   - Remove prefixos das colunas e realiza transformaÃ§Ãµes.
   - Salva os dados tratados na camada *Silver*.

3. **Camada Silver â†’ Gold (`gold`)**:  
   - Realiza junÃ§Ãµes e agregaÃ§Ãµes nos dados da Silver.
   - Salva os resultados analÃ­ticos na camada *Gold*.

> As tarefas de Bronze e Silver sÃ£o executadas **em paralelo**, e a camada Gold sÃ³ Ã© processada apÃ³s ambas serem concluÃ­das.


## â–¶ï¸ ExecuÃ§Ã£o

### PrÃ©-requisitos

- Apache Airflow instalado e configurado.
- PySpark disponÃ­vel no ambiente.
- Estrutura de diretÃ³rios esperada:
O projeto assume a seguinte estrutura de pastas dentro do diretÃ³rio base (resolvido dinamicamente no cÃ³digo):

<pre>
dados/
â”œâ”€â”€ landing/
â”œâ”€â”€ bronze/
â”œâ”€â”€ silver/
â””â”€â”€ gold/
</pre>

âš™ï¸ O caminho base Ã© detectado dinamicamente no cÃ³digo com:
```
base_path = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
```

### Ativando o Airflow

```bash
# Inicialize o Airflow
airflow db init
airflow scheduler
airflow webserver --port 8080
```

Acesse a interface: http://localhost:8080

### Visualizando o Pipeline

Na UI do Airflow, vocÃª verÃ¡ a DAG:

- pipeline_lakehouse âœ… (orquestradora principal)

VocÃª pode ativar e rodar a pipeline_lakehouse para executar o pipeline completo.

### ğŸ‘©â€ğŸ’» Autoria
Desenvolvido por Bruna Silveira
